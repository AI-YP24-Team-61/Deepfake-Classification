{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_preprocessing import CustomModel, data_pipeline, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"D:\\\\1_Магистратура_ВШЭ\\\\1_AI_YP24_Team_61\\\\Deepfake-Classification\\\\app\\\\data\\\\cifake-real-and-ai-generated-synthetic-images\"\n",
    "image_datasets_logreg = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                                 data_transforms[x]\n",
    "                                                 ) for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "evens = list(range(0, len(image_datasets_logreg['train']), 2))\n",
    "train_dataset = torch.utils.data.Subset(image_datasets_logreg['train'], evens)\n",
    "\n",
    "evens = list(range(0, len(image_datasets_logreg['test']), 2))\n",
    "test_dataset = torch.utils.data.Subset(image_datasets_logreg['test'], evens)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "\t\t\t\t\t\t\t\t\t\t   batch_size=512,\n",
    "\t\t\t\t\t\t\t\t\t\t   shuffle=True, \n",
    "\t\t\t\t\t\t\t\t\t\t   num_workers=0, \n",
    "\t\t\t\t\t\t\t\t\t\t   pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "\t\t\t\t\t\t\t\t\t\t   batch_size=512,\n",
    "\t\t\t\t\t\t\t\t\t\t   shuffle=True, \n",
    "\t\t\t\t\t\t\t\t\t\t   num_workers=0, \n",
    "\t\t\t\t\t\t\t\t\t\t   pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {x: len(train_dataset) if x == 'train' else len(test_dataset) for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 50000, 'test': 10000}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {x: len(image_datasets_logreg[x]) for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FAKE', 'REAL']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets_logreg['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FAKE': 0, 'REAL': 1}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets_logreg['train'].class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [i for i in range(len(image_datasets_logreg['train'])) if image_datasets_logreg['train'][i][1] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49999"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пробуем сделать inference готовой моделью и сделать пайплайн трансформации одного изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_preprocessing import CustomModel, data_pipeline, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomModel(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (loss): BCELoss()\n",
      ")\n",
      "CustomModel(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (loss): BCELoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CustomModel()\n",
    "print(model)\n",
    "\n",
    "model_path = f\"D:\\\\1_Магистратура_ВШЭ\\\\1_AI_YP24_Team_61\\\\Deepfake-Classification\\\\app\\\\backend\\\\model_weights\\\\string.pt\"\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "print(model)\n",
    "model.eval()\n",
    "\n",
    "#im = Image.open(r\"..\\data\\inference_image\\1.webp\")\n",
    "im = Image.open(r\"..\\data\\inference_image\\3.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo():\n",
    "\treturn [1, 2]\n",
    "\n",
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "im_tensor = transform(im)\n",
    "im_tensor = im_tensor.reshape(-1, 3, 32, 32)\n",
    "im_tensor = im_tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(im_tensor.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8659921884536743, True]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(im_tensor)\n",
    "[float(pred[-1][-1]), float(pred[-1][-1]) > 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(im_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "H, W = 32, 32\n",
    "img = torch.randint(0, 256, size=(3, H, W), dtype=torch.uint8)\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "img = transforms(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scienceplots\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "\n",
    "from contextlib import nullcontext\n",
    "from functools import partialmethod\n",
    "from pprint import pp\n",
    "from pytorch_grad_cam import AblationCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:22<00:00, 4352.42it/s]\n",
      "100%|██████████| 20000/20000 [00:04<00:00, 4339.60it/s]\n"
     ]
    }
   ],
   "source": [
    "widths = []\n",
    "heights = []\n",
    "for i_split in ['train', 'test']: # произведем замеры всех изображения (на train и test)\n",
    "    if i_split == 'train':\n",
    "        len_train = len(data[i_split])\n",
    "    else:\n",
    "        len_test = len(data[i_split])\n",
    "    for i_img in tqdm(data[i_split]):\n",
    "        image = i_img[0]\n",
    "        image = torch.permute(image, (1, 2, 0))\n",
    "        widths.append(image.shape[0]) # заносим в список параметры широты всех изображений\n",
    "        heights.append(image.shape[1]) # заносим в список параметры широты всех изображений  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(102400000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(widths[:len_train]) * np.array(heights[:len_train])).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102400000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cntTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:25<00:00, 3972.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# тензоры для хранения прокси-информации по RGB каналам\n",
    "tsum = torch.tensor([0.0, 0.0, 0.0])\n",
    "tsum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "cntTrain = (np.array(widths[:len_train]) * np.array(heights[:len_train])).sum() # количество пикселей в выборке\n",
    "\n",
    "for i_img in tqdm(data['train']):\n",
    "    tsum += i_img[0].sum(axis=(1, 2))\n",
    "    tsum_sq += (i_img[0]**2).sum(axis=(1, 2))\n",
    "    \n",
    "mean_rgb = tsum / cntTrain\n",
    "var_rgb = (tsum_sq / cntTrain) - (mean_rgb**2) # E(X^2) - E^2(X)\n",
    "std_rgb = torch.sqrt(var_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4720, 0.4629, 0.4178]),\n",
       " tensor([0.0564, 0.0564, 0.0707]),\n",
       " tensor([0.2376, 0.2374, 0.2659]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rgb, var_rgb, std_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4720, 0.4629, 0.4178]),\n",
       " tensor([0.0564, 0.0564, 0.0707]),\n",
       " tensor([0.2376, 0.2374, 0.2659]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rgb, var_rgb, std_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_min_max_size(data):\n",
    "    widths = []\n",
    "    heights = []\n",
    "    for i_split in ['train', 'test']: # произведем замеры всех изображения (на train и test)\n",
    "        if i_split == 'train':\n",
    "            len_train = len(data[i_split])\n",
    "        else:\n",
    "            len_test = len(data[i_split])\n",
    "        for i_img in tqdm(data[i_split]):\n",
    "            image = i_img[0]\n",
    "            image = torch.permute(image, (1, 2, 0))\n",
    "            widths.append(image.shape[0]) # заносим в список параметры широты всех изображений\n",
    "            heights.append(image.shape[1]) # заносим в список параметры широты всех изображений \n",
    "\n",
    "    # Делаем расчет средних, минимальных и максимальных размеров изображений\n",
    "    array_sizes = np.multiply(widths, heights) # Создаем массив всех размеров (width*heights)\n",
    "    avg_size = np.mean(array_sizes)\n",
    "    min_size = np.min(array_sizes)\n",
    "    max_size = np.max(array_sizes)\n",
    "\n",
    "    return avg_size, min_size, max_size, widths, heights, len_train, len_test\n",
    "\n",
    "def calc_mean_std(data, widths, heights, segment, len_train, len_test):\n",
    "    # тензоры для хранения прокси-информации по RGB каналам\n",
    "    tsum = torch.tensor([0.0, 0.0, 0.0])\n",
    "    tsum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "    if segment == 'train':\n",
    "        cntTrain = (np.array(widths[:len_train]) * np.array(heights[:len_train])).sum() # количество пикселей в выборке\n",
    "    if segment == 'test':\n",
    "        cntTrain = (np.array(widths[len_test:]) * np.array(heights[len_test:])).sum() # количество пикселей в выборке\n",
    "\n",
    "    for i_img in tqdm(data['train']):\n",
    "        tsum += i_img[0].sum(axis=(1, 2))\n",
    "        tsum_sq += (i_img[0]**2).sum(axis=(1, 2))\n",
    "        \n",
    "    mean_rgb = tsum / cntTrain\n",
    "    var_rgb = (tsum_sq / cntTrain) - (mean_rgb**2) # E(X^2) - E^2(X)\n",
    "    std_rgb = torch.sqrt(var_rgb)\n",
    "\n",
    "    return mean_rgb, var_rgb, std_rgb\n",
    "\n",
    "def target_hist(data):\n",
    "    dct_targetHist = {}\n",
    "\n",
    "    for i_split in ['train', 'test']:\n",
    "        cnt_fake = 0\n",
    "        cnt_real = 0\n",
    "        for _, target in tqdm(data[i_split]):\n",
    "            if target == 1:\n",
    "                cnt_real += 1\n",
    "            elif target == 0: \n",
    "                cnt_fake += 1\n",
    "        dct_targetHist[i_split] = dct_targetHist.get(i_split, {'fake': cnt_fake, \n",
    "                                                            'real': cnt_real})\n",
    "    return dct_targetHist\n",
    "\n",
    "def make_eda():\n",
    "    # Предварительная загрузка и трансформация данных из локальной папки на ПК\n",
    "    data = {\n",
    "        # Создаем словарь, где по ключу train/test будут лежать соответствующие данные\n",
    "        # По очереди обращаемся к папкам train/test, чтобы загрузить оттуда данные\n",
    "        split: datasets.ImageFolder(\n",
    "            # Если путь f\"{DATASET_DIR}/{split}\" содержит другие папки, то названия этих папок устанавливаются в качестве\n",
    "            # классов (LABELS) для данных в папке f\"{DATASET_DIR}/{split}\".\n",
    "            # Узнать метки классов можно с помощью вызова data['train'].classes\n",
    "            fr\"..\\data\\dataset_example\\{split}\",\n",
    "            #\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                ]\n",
    "            ),\n",
    "            #target_transform=TargetMapper(LABELS),\n",
    "        )\n",
    "        for split in ['train', 'test']\n",
    "    }\n",
    "    print(\"Данные загружены!\")\n",
    "    # Считаем средний, минимальный и максимальный размер изображений во всем датасете\n",
    "    avg_size, min_size, max_size, widths, heights, len_train, len_test = calc_min_max_size(data)\n",
    "    # Считаем mean/std в разрезе train/test\n",
    "    mean_rgb_train, var_rgb_train, std_rgb_train = calc_mean_std(data, widths, heights, 'train', len_train, len_test)\n",
    "    mean_rgb_test, var_rgb_test, std_rgb_test = calc_mean_std(data, widths, heights, 'test', len_train, len_test)\n",
    "    dct_targetHist = target_hist(data)\n",
    "\n",
    "    return {'avg_size': avg_size, \n",
    "            'min_size': min_size, \n",
    "            'max_size': max_size,\n",
    "            'mean_rgb_train': mean_rgb_train, \n",
    "            'var_rgb_train': var_rgb_train, \n",
    "            'std_rgb_train': std_rgb_train, \n",
    "            'mean_rgb_test': mean_rgb_test, \n",
    "            'var_rgb_test': var_rgb_test, \n",
    "            'std_rgb_test': std_rgb_test,\n",
    "            'dct_targetHist': dct_targetHist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_preprocessing import CustomModel, data_pipeline, train_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные загружены!\n"
     ]
    }
   ],
   "source": [
    "# Предварительная загрузка и трансформация данных из локальной папки на ПК\n",
    "data = {\n",
    "    # Создаем словарь, где по ключу train/test будут лежать соответствующие данные\n",
    "    # По очереди обращаемся к папкам train/test, чтобы загрузить оттуда данные\n",
    "    split: datasets.ImageFolder(\n",
    "        # Если путь f\"{DATASET_DIR}/{split}\" содержит другие папки, то названия этих папок устанавливаются в качестве\n",
    "        # классов (LABELS) для данных в папке f\"{DATASET_DIR}/{split}\".\n",
    "        # Узнать метки классов можно с помощью вызова data['train'].classes\n",
    "        fr\"..\\data\\dataset_example\\{split}\",\n",
    "        #\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    for split in ['train', 'test']\n",
    "}\n",
    "\n",
    "print(\"Данные загружены!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(data):\n",
    "\tdct_targetHist = {}\n",
    "\tfor i_split in ['train', 'test']:\n",
    "\t\tif i_split == 'train':\n",
    "\t\t\tlen_train = len(data[i_split])\n",
    "\t\telse:\n",
    "\t\t\tlen_test = len(data[i_split])\n",
    "\t\t# Тензоры для хранения прокси-информации по RGB каналам\n",
    "\t\ttsum = torch.tensor([0.0, 0.0, 0.0])\n",
    "\t\ttsum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\t\t# Переменные для расчета числа наблюдений в каждом классе\n",
    "\t\tcnt_fake = 0\n",
    "\t\tcnt_real = 0\n",
    "\t\t# Инициализируем списки для расчета статистик по размерам\n",
    "\t\twidths = []\n",
    "\t\theights =[]\n",
    "\t\tfor image, target in tqdm(data[i_split]):\n",
    "\t\t\t# Прокси-расчеты для вычисления статистики по размерам изобрежений\n",
    "\t\t\timage_sz = torch.permute(image, (1, 2, 0))\n",
    "\t\t\twidths.append(image_sz.shape[0]) # заносим в список параметры широты всех изображений\n",
    "\t\t\theights.append(image_sz.shape[1]) # заносим в список параметры широты всех изображений\n",
    "\t\t\t# Прокси-расчеты для вычисления статистики по mean/std\n",
    "\t\t\ttsum += image.sum(axis=(1, 2))\n",
    "\t\t\ttsum_sq += (image**2).sum(axis=(1, 2))\n",
    "\t\t\t# Считаем число экземпляров каждого класса в разрезе train/test\n",
    "\t\t\tif target == 1:\n",
    "\t\t\t\tcnt_real += 1\n",
    "\t\t\telif target == 0: \n",
    "\t\t\t\tcnt_fake += 1\n",
    "\t\tif i_split == 'train':\n",
    "\t\t\tcntPixels = (np.array(widths) * np.array(heights)).sum() # количество пикселей в выборке\n",
    "\t\telif i_split == 'test':\n",
    "\t\t\tcntPixels = (np.array(widths) * np.array(heights)).sum() # количество пикселей в выборке\n",
    "\t\t# Считаем статистики по размерам изображений\n",
    "\t\tarray_sizes = np.multiply(widths, heights) # Создаем массив всех размеров (width*heights)\n",
    "\t\tavg_size = np.mean(array_sizes)\n",
    "\t\tmin_size = np.min(array_sizes)\n",
    "\t\tmax_size = np.max(array_sizes)\n",
    "\t\t# Считаем mean/std в разрезе каждого класса\n",
    "\t\tmean_rgb = tsum / cntPixels\n",
    "\t\tvar_rgb = (tsum_sq / cntPixels) - (mean_rgb**2) # E(X^2) - E^2(X)\n",
    "\t\tstd_rgb = torch.sqrt(var_rgb)\n",
    "\t\tdct_targetHist[i_split] = dct_targetHist.get(i_split, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'fake_cnt': cnt_fake,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'real_cnt': cnt_real,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'avg_size': float(avg_size),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'min_size': float(min_size),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'max_size': float(max_size),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'mean_rgb': mean_rgb.tolist(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'var_rgb': var_rgb.tolist(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'std_rgb': std_rgb.tolist()\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t})\n",
    "\treturn dct_targetHist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:24<00:00, 4002.91it/s]\n",
      "100%|██████████| 20000/20000 [00:04<00:00, 4012.91it/s]\n"
     ]
    }
   ],
   "source": [
    "tt = calculate_statistics(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df.T.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['mean_red'] = dft['mean_rgb'].apply(lambda x: x[0])\n",
    "dft['mean_green'] = dft['mean_rgb'].apply(lambda x: x[1])\n",
    "dft['mean_blue'] = dft['mean_rgb'].apply(lambda x: x[2])\n",
    "\n",
    "dft['std_red'] = dft['std_rgb'].apply(lambda x: x[0])\n",
    "dft['std_green'] = dft['std_rgb'].apply(lambda x: x[1])\n",
    "dft['std_blue'] = dft['std_rgb'].apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fake_cnt</th>\n",
       "      <td>50000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real_cnt</th>\n",
       "      <td>50000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_size</th>\n",
       "      <td>1024.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_size</th>\n",
       "      <td>1024.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_size</th>\n",
       "      <td>1024.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_red</th>\n",
       "      <td>0.471996</td>\n",
       "      <td>0.473014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_green</th>\n",
       "      <td>0.462891</td>\n",
       "      <td>0.463721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_blue</th>\n",
       "      <td>0.417822</td>\n",
       "      <td>0.418606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train      test\n",
       "fake_cnt       50000     10000\n",
       "real_cnt       50000     10000\n",
       "avg_size      1024.0    1024.0\n",
       "min_size      1024.0    1024.0\n",
       "max_size      1024.0    1024.0\n",
       "mean_red    0.471996  0.473014\n",
       "mean_green  0.462891  0.463721\n",
       "mean_blue   0.417822  0.418606"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft[['fake_cnt', 'real_cnt', 'avg_size', 'min_size', 'max_size', 'mean_red', 'mean_green', 'mean_blue', 'std_red', 'std_green', 'std_blue']].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fake_cnt', 'real_cnt', 'avg_size', 'min_size', 'max_size', 'mean_rgb',\n",
       "       'var_rgb', 'std_rgb', 'mean_red', 'mean_green', 'mean_blue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
